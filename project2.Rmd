---
title: "Practical Machine Learning Project"
author: "Reid the scientist"
date: "September 21, 2014"
output: html_document
---

###Using the Generalized Boosted Regression Model (GBM) to predict a multigroup categorical variable.  

####First, let's read in the data

```{r, cache=TRUE, results='hide'}
library(modeest)
library(caret)
require(data.table)

setwd("/Users/Reid/OneDrive/R/Coursera/course materials/practical machine learning/project/")

training<-read.csv("pml-training.csv", na.strings=c("NA",""))
testing<-read.csv("pml-testing.csv", na.strings=c("NA",""))

inTrain <- createDataPartition(training$classe, p=0.75, list = FALSE)
train <- training[inTrain,]
pretest <- training[-inTrain,]
````

####Next, let's drop the annoying variables, 

######names, row numbers and timestamps that I found didn't help much. 

####Also, dropping all the rows with more than 10k NA values. 
````{r, cache=TRUE}
t1<-train[, -(1:6)]
p1<-pretest[, -(1:6)]
test1<-testing[, -(1:6)]

#this names the columns with lots of NAs
getrid<-which(colSums(is.na(t1)) > 10000)

#this removes those columns
t2<-t1[, -getrid]
p1<-p1[, -getrid]
test1<-test1[, -getrid]

#don't forget to get rid of the classe var!

t3<-t2[, -54]
p3<-p1[, -54]


```

#####NOTE:
I first tied reducing the variables with a PCA, but it was messy and I only got
71% accuracy. So, then I tried a gbm model with the raw data 
(the exact model, of course, took some trial and error). Random forests took 
way too long and a multinomial logistic regression was not accurate at all.  

```{r, cache=TRUE, results='hide'}

#testing this model now
modelFitGBM_noPCA <- train(t2$classe~ .,method="gbm", data=t3)

predict_train<-predict(modelFitGBM_noPCA, t3)
predict_pretest<-predict(modelFitGBM_noPCA, p3)
predict_Test<-predict(modelFitGBM_noPCA, test1)

```


#####Let's create some confusion matrixes (matrices?)
######First, for the training dataset
```{r, cache=TRUE}

tableTrain<-table(predict_train, t2$classe)
tablePretest<-table(predict_pretest, p1$classe)
#tableTest<-table(predict_train, test1$classe)

confusionMatrix(tableTrain)
```

####In sample error rate: 0.007
The in-sample Accuracy is 0.993, or 99.3%. Not bad!
So, the in-sample error is about 0.007 or 0.7%. 

Do the authors describe what they expect the out of sample error to be and estimate the error appropriately with cross-validation?

#### Now let's check out the *pretest* dataset, to estimate out-of-sample error. 
Here, we're using the same model as above to estaimate the classes on a subsample of the training set. This subsample, which I'm calling the pretest, was not used to estimate the model. 

#### Confusion Matrix!
#####For brevity, I'm just showing the Overall Statistics section 

```{r, cache=TRUE}
confusionMatrix(tablePretest)[3]

```

###Out of sample error rate: 0.0118

Out of sample accuracy estimated at 0.9882, or 98.82%
So that means the out-of-sample error is about 0.0118, or 1.18%

#### Based on the above model, here are the predictions for the test dataset:

```{r, cache=TRUE}

predict_Test

```

